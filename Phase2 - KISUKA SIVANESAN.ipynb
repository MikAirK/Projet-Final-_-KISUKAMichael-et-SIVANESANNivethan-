{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Filière :* ING3 - Tronc commun\n",
    "\n",
    "*Auteurs :*\n",
    "**Michael KISUKA** et\n",
    "**Nivethan SIVANESAN**\n",
    "\n",
    "*Professeur référent :* \n",
    "**TAJINI Badr**\n",
    "\n",
    "*Année :* 2024-2025\n",
    "\n",
    "**ESIEE-IT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet : Classification de chiffres manuscrits (MNIST) avec MLP\n",
    "\n",
    "## Phase 2 : Construction d'un modèle MLP basique\n",
    "\n",
    "### Objectif\n",
    "L'objectif de cette phase est de se familiariser avec la construction d'un modèle MLP (Multi-Layer Perceptron) en utilisant le dataset MNIST. Les étapes incluent :\n",
    "- **Chargement des données** : Comprendre la structure des données MNIST.\n",
    "- **Préparation des données** : Normalisation des valeurs des pixels.\n",
    "- **Construction du modèle** : Instanciation d'un modèle MLP avec différents hyperparamètres.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étapes\n",
    "\n",
    " **1. Chargement des données MNIST**\n",
    " \n",
    "Le dataset MNIST contient 70 000 images de chiffres manuscrits (de 0 à 9), chacune étant une image en niveaux de gris de 28x28 pixels. Ces images sont aplaties en vecteurs de 784 pixels pour être utilisées par le modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "# Charger le dataset MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "# Examiner les données\n",
    "print(\"Forme des données d'images (X) :\", X.shape)  # (70000, 784)\n",
    "print(\"Forme des étiquettes (y) :\", y.shape)       # (70000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Préparation des données**\n",
    "\n",
    "Pour améliorer l’apprentissage, on normalise les données en divisant les valeurs des pixels par 255.0, ce qui ramène les valeurs entre 0 et 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des données\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Construction du modèle MLP**\n",
    "\n",
    "Plusieurs modèles MLP sont instanciés avec différentes configurations pour explorer l'impact des hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Exemple 1 : Un MLP très simple avec une seule couche cachée de 50 neurones\n",
    "mlp_simple = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10)\n",
    "print(\"Modèle MLP simple créé :\", mlp_simple)\n",
    "\n",
    "# Exemple 2 : Un MLP avec deux couches cachées\n",
    "mlp_deux_couches = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=10)\n",
    "print(\"Modèle MLP à deux couches créé :\", mlp_deux_couches)\n",
    "\n",
    "# Exemple 3 : Essayer différents algorithmes d'optimisation\n",
    "mlp_adam = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', max_iter=10)\n",
    "print(\"MLP avec optimiseur Adam :\", mlp_adam)\n",
    "\n",
    "mlp_sgd = MLPClassifier(hidden_layer_sizes=(50,), solver='sgd', learning_rate_init=0.01, max_iter=10)\n",
    "print(\"MLP avec optimiseur SGD :\", mlp_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le code final :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme des données d'images (X) : (70000, 784)\n",
      "Forme des étiquettes (y) : (70000,)\n",
      "Modèle MLP simple créé : MLPClassifier(hidden_layer_sizes=(50,), max_iter=10)\n",
      "Modèle MLP à deux couches créé : MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=10)\n",
      "MLP avec optimiseur Adam : MLPClassifier(hidden_layer_sizes=(50,), max_iter=10)\n",
      "MLP avec optimiseur SGD : MLPClassifier(hidden_layer_sizes=(50,), learning_rate_init=0.01, max_iter=10,\n",
      "              solver='sgd')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# 1. Charger le dataset MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "# 2. Examiner les données (pour comprendre leur forme)\n",
    "print(\"Forme des données d'images (X) :\", X.shape)\n",
    "print(\"Forme des étiquettes (y) :\", y.shape)\n",
    "\n",
    "# 3. Préparer les données (une étape simple : mise à l'échelle - peut être simplifiée au début)\n",
    "#    Note : Pour simplifier au maximum, on pourrait même sauter cette étape au début.\n",
    "X = X / 255.0\n",
    "\n",
    "# 4. Construire un modèle MLP basique\n",
    "#    Ici, nous allons juste instancier le modèle avec différents paramètres.\n",
    "\n",
    "# Exemple 1 : Un MLP très simple avec une seule couche cachée de 50 neurones\n",
    "mlp_simple = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10)\n",
    "print(\"Modèle MLP simple créé :\", mlp_simple)\n",
    "\n",
    "# Exemple 2 : Un MLP avec deux couches cachées\n",
    "mlp_deux_couches = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=10)\n",
    "print(\"Modèle MLP à deux couches créé :\", mlp_deux_couches)\n",
    "\n",
    "# Exemple 3 : Essayer différents algorithmes d'optimisation\n",
    "mlp_adam = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', max_iter=10)\n",
    "print(\"MLP avec optimiseur Adam :\", mlp_adam)\n",
    "\n",
    "mlp_sgd = MLPClassifier(hidden_layer_sizes=(50,), solver='sgd', learning_rate_init=0.01, max_iter=10)\n",
    "print(\"MLP avec optimiseur SGD :\", mlp_sgd)\n",
    "\n",
    "# Remarque : `max_iter` est limité ici pour éviter que l'entraînement ne prenne trop de temps si on l'exécute.\n",
    "# L'objectif principal est l'instanciation du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des résultats\n",
    "\n",
    "**1. Forme des données**\n",
    "\n",
    "X.shape : (70000, 784) → 70 000 images avec 784 caractéristiques chacune (les pixels).\n",
    "\n",
    "y.shape : (70000,) → 70 000 étiquettes correspondant aux chiffres de 0 à 9.\n",
    "\n",
    "**2. Impact des hyperparamètres**\n",
    "\n",
    "hidden_layer_sizes=(50,) : Une seule couche cachée avec 50 neurones.\n",
    "\n",
    "hidden_layer_sizes=(100, 50) : Deux couches cachées avec 100 et 50 neurones.\n",
    "\n",
    "solver='adam' : Algorithme souvent efficace, qui ajuste automatiquement le taux d’apprentissage.\n",
    "\n",
    "solver='sgd' : Descente de gradient stochastique, qui nécessite un réglage manuel du taux d’apprentissage (learning_rate_init).\n",
    "\n",
    "**3. Remarques**\n",
    "\n",
    "Ajouter plus de neurones et de couches peut améliorer la capacité du modèle à apprendre des relations complexes, mais cela rend l'entraînement plus long et exige plus de puissance de calcul.\n",
    "\n",
    "L’argument max_iter=10 limite le nombre d’itérations d’apprentissage pour éviter un temps de calcul trop long lors des tests. En pratique, un modèle bien entraîné nécessite plusieurs centaines d’itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Cette phase permet d'expérimenter avec différentes architectures et méthodes d’optimisation pour mieux comprendre le fonctionnement des réseaux de neurones avant de les entraîner réellement. La prochaine étape consistera à entraîner ces modèles et à évaluer leurs performances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
